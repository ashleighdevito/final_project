<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>The Project</title>

  <!-- D3 -->
  <script src="https://d3js.org/d3.v5.min.js"></script>

  <!-- Plotly -->
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

  <!-- Our CSS -->
  <link rel="stylesheet" type="text/css" href="../static/css/style.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
</head>

<body>
  <!-- navbar -->
  <nav class="navbar sticky-top navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid bar">
      <a class="navbar-brand main-button" href="/index">Predicting Movie Box Office</a>
      <ul class="navbar-nav ms-auto">
        <li class="nav-item ms-auto">
          <a href="/model" class="nav-link" style="color: rgba(205, 214, 219, 0.925)">The Project</a>
        </li>
        <li class="nav-item ms-auto">
          <a href="/team" class="nav-link" style="color: rgba(205, 214, 219, 0.925)">Our Team</a>
        </li>
        <li class="nav-item ms-auto">
          <a target="_blank" href="https://sdinespierce.github.io/etl_project/" class="nav-link" style="color: rgba(205, 214, 219, 0.925)">ETL Project</a>
        </li>
      </ul>
    </div>
  </nav>
  <!-- main page -->
  <main>
    <div class="container-fluid">
      <div class='row'>
        <div class="col-10 offset-1" id="frame">
          <div class='row'>
            <h1>The Project</h1>
          </div>
          <div class="row">
            <h2>Goal</h2><br>
            <p>
            Big-budget movies carry a large financial risk.  In order to help make decisions on release strategy, we created an application to predict the box office success of a film.<br>
            We gathered data on historical movie releases from the past 15 years to create an algorithm and predict a range of box office earnings for future debuts.<br>
            </p>
            <br><h2>Approach</h2><br>
            <p>
            We followed a traditional extract, transform, and load process to create a database hosted on Amazon Web Services (AWS).  The data we collected includes budget, runtime, genres, directors, actors, critical ratings, and more. The full list of included features can be seen in the data dictionary. 
            </p>
            <p>
            We gathered data regarding releases over the past 15 years (2006-present) as we considered that to be the modern timeframe with consistent marketing strategies. The target movie set was determined to be films releasing over the next several months.  
            </p>
            <br><h2>Process</h2><br>
            <br><h3>Extraction:</h3><br>
            <p>
            Movie titles and budget data was gathered from thenumbers.com<br>
            The majority of historical data was called from the OMDb API.<br>
            The data on movies that were to be released was gathered from IMDb.<br>
            Buzz, our metric for marketing, was gathered through Google Trends analysis of the movie titles.<br>
            Web scrapers were used to gather non-API data.              
            </p>
            <br><h3>Transformation:</h3><br>
            <p>
            Data was cleaned using Jupyter notebook scripts.<br>
            All of the data gathered from OMDb was stored as objects, so all datatypes were converted appropriately to load into the database.<br>
            Movie titles for the upcoming released films scraped from IMDb included year of release in the string. This attribute was removed via string split.<br>
            Omitted data included: studio, language, plot, tagline, directors, writers, actors.<br>
            Release date information was converted to Month.<br>
            Features gathered from separate sources were all merged to a single dataframe on the movie title.<br>
            
            </p>
            <br><h3>Loading:</h3><br>
            <p>
            CSVs containing the final cleaned data were imported into the AWS database via Jupyter notebook scripts connecting to PostgreSQL. <br>
            pgAdmin was our preferred client.              
            </p>
            <br><h3>Modeling:</h3><br>
            <p>
            Data was called from our database and used to create an XG boost model.<br>
            Dummy columns were created for genre and movie rating.<br>
            The historical dataset was split into testing and training sets.<br>
            Once optimized, the model was exported, allowing it to be accessible to our application.            
            </p>
            <br><h3>Presenting:</h3><br>
            <p>
            Our model was deployed using a Flask application, built with Python, which applies our model to data describing upcoming movies.<br>
            A website, hosted through Heroku, creates an interface in which a user is able to select a film to view our prediction for that film’s Box Office sales.
            </p>
            <br><h2>Product</h2><br>
            <p>
            Our application contains pages for prediction, a project analysis, a team profile, as well as a link to a previous project.
            </p>
            <br><h2>Challenges</h2><br>
            <br><h3>In data collection:</h3><br>
            <p>
            A movie’s budget was one of the biggest indicators of box office success and most of that data is behind a significant paywall. The budget data we could find pared our dataset down to 25% of its original size. This also lead to an issue in the availability of budget data for the movies we are trying to predict. We solved this by either using available data, or estimating the budget as a median value of the training data. 
            </p>
            <p>
            Another sticky wicket was trying to quantify the success of marketing campaigns, which no doubt have an impact on box office gross. We decided to create a feature called Buzz that looked at google trend data for the year prior to a movie’s release date. For each week the search term (<movie name> + “movie”) was scored on a scale of 0-100. We counted the weeks that the score was greater than 20 and used that number as our Buzz feature. One issue with this method was that movies coming out far in the future wouldn’t have an accurate score since they haven’t reached their peak search popularity, which is almost always the week of the movie’s release.
            </p>
            <br><h3>In data cleaning:</h3><br>
            <p>
            As we moved into the data cleaning portion of the project, the shape of our data began to change. We encountered noisy, inconsistent, and missing data. For these reasons we elected to not include the studio, language, plot, or tagline of the movies. We also decided to drop directors, writers, and actors from the data. While they could be a good indicator of a movie’s financial success, they greatly affected the accuracy of the model. After dropping irrelevant columns, changing the release date to a month column, creating dummy columns for genre and movie rating, and scaling the data, we were ready to test out some models.
            </p>
            <br><h3>In modeling:</h3><br>
            <p>
            Our neural network model didn’t work with the pickling so we saved it as an h5 file. This proved tricky to work with and we decided to stick to using the random forest and xg boost models. Ultimately, we chose the XG boost model to use in our application.
            </p>
            <p>
            Initially, we tried multiple models and target data binning methods to see which ones would yield the best result. Linear regression and SVM models didn’t seem suitable for the dataset that we assembled, so we moved on to random forests, boosted models, and neural networks. These 3 models yielded similar accuracy scores. When grouping the models into 15 bins, all models had an accuracy score in the 30% range. While we liked the specific targets, we deemed the accuracy too low and went about reducing the number and range of the bins. We were able to achieve an accuracy rating of up to 95% by changing the way we binned the target data. After much trial and error we settled on using 4 bins for our target data: 0-20 million, 20-100 million, 100-200 million, and over 200 million USD. This model resulted in an accuracy score of 64% but we feel that it has more meaningful predictions.
            </p>
          </div>
        </div>
      </div>  
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
</body>
</html>